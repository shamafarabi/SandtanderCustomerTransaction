{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shamabar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Importing Packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Importing packages for cross_validation \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#for modeling \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "\n",
    "#for feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "## Grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/shamabar/Desktop/Data Analysis Skills/Kaggle/Santander/train.csv')\n",
    "test=pd.read_csv('C:/Users/shamabar/Desktop/Data Analysis Skills/Kaggle/Santander/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Inspection\n",
    "\n",
    "*  The goal of this step is to check if the features are numerical/categorical and also if they have any missing values. Findings from the inspection helps to determine how the data should be processed before applying modeling.\n",
    "\n",
    "*  Findings from the code below show that 'train' and test dataset has 201 input features and 200,000 entries, and no nulls values in the dataset. This means that  the solution approach does not require any of the pre- processing steps that are usually followed for handling missing values in a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Inpsect Number of Entries,Features and Data Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>train_199995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>train_199997</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>train_199999</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "199995  train_199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404   \n",
       "199996  train_199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096   \n",
       "199997  train_199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410   \n",
       "199998  train_199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   \n",
       "199999  train_199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   \n",
       "\n",
       "          var_5   var_6    var_7   ...     var_190  var_191  var_192  var_193  \\\n",
       "199995  11.6081  5.6709  15.1516   ...      6.1415  13.2305   3.9901   0.9388   \n",
       "199996 -10.5628  5.8802  21.5940   ...      4.9611   4.6549   0.6998   1.8341   \n",
       "199997  -5.4086  4.5555  21.5571   ...      4.0651   5.4414   3.1032   4.8793   \n",
       "199998   0.5339  6.0479  17.0152   ...      2.6840   8.6587   2.7337  11.1178   \n",
       "199999   0.3488  5.2839  15.2058   ...      8.9842   1.6893   0.1276   0.3766   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "199995  18.0249  -1.7939   2.1661   8.5326  16.6660 -17.8661  \n",
       "199996  22.2717   1.7337  -2.1651   6.7419  15.9054   0.3388  \n",
       "199997  23.5311  -1.5736   1.2832   8.7155  13.8329   4.1995  \n",
       "199998  20.4158  -0.0786   6.7980  10.0342  15.5289 -13.9001  \n",
       "199999  15.2101  -2.4907  -2.2342   8.1857  12.1284   0.1385  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  18.2675  2.1337   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   \n",
       "1  18.6316 -4.4131   ...     10.6165   8.8349   0.9403  10.1282  15.5765   \n",
       "2  20.2537  1.5233   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   \n",
       "3  20.5660  3.3755   ...      9.5702   9.0766   1.6580   3.5813  15.1874   \n",
       "4  10.6048  2.9890   ...      4.2259   9.1723   1.2835   3.3778  19.5542   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 201 entries, ID_code to var_199\n",
      "dtypes: float64(200), object(1)\n",
      "memory usage: 306.7+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of Missing Values in the Datase**t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection\n",
    "\n",
    "* [Feature selection algorithms](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) help to determine which features significantly influence the output variable and then eliminate the remaining  features from consideration. This step was useful as this dataset has large number of features, being able to eliminate features that have little to no impact on the analysis reduced the complexity of the model and enabled machine learning algortihms to train faster. \n",
    "\n",
    "* The scikit module provides [several ways](https://scikit-learn.org/stable/modules/feature_selection.html) for identifying significant features either by evaluating the statistical correlation with the outcome variable (e.g. univariate feature selection method) or by measuring  the usefulness of a subset of feature by actually training a machine learning model on it (e.g. recursive feature elimination). \n",
    "\n",
    "* Considering the size of this particular dataset and the computational time usually required for recursive feature elimination, the universal feature seleciton method was adopted to select the top 25 features that are critical for modeling. Later, submissions were also made by selecting top 50, 100 and 150 features to check the effect of feature selection on accuracy and time required for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Define Target Variable and Test Variables for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X= train.drop(columns=['target','ID_code'],axis=1)\n",
    "train_Y=train['target']\n",
    "test_X= test.drop(columns=['ID_code'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Select Features to be Considered for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new=selector.fit_transform(train_X, train_Y)\n",
    "mask = selector.get_support(indices=True)\n",
    "colname = train_X.columns[mask]# Names of the selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Redifining Train and  Test Data Using the Selected Features** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_new = train_X[colname]\n",
    "test_X_new = test_X[colname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Different Classification Models using Selected Features\n",
    "\n",
    "* All models (KNN, Logistic Regresson, Decision Tree,XGBoost and Random Forest) were trained using the train dataset with selected features. \n",
    "* To estimate the accuracy of these models when applied to unseen data, cross validation was performed for each model by splitting the train dataset (80%/20% for train/test split), fitting the model and computing the score 5 consecutive times (with different splits each time) based on ROC_AUC score. The mean of the accuracies for the 5 trials were then reported as the CV score for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Defining models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest, Decision Tree, Logistic Regression and K Nearest Neighbours, and XGBoost models\n",
    "RFmodel=RandomForestClassifier(random_state=1)\n",
    "DTmodel=DecisionTreeClassifier(random_state=1)\n",
    "logreg = LogisticRegression()\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "xgbm = xgb.XGBClassifier(silent=True, scale_pos_weight=1,learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=1000, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=4, \n",
    "                      gamma=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training the Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4, gamma=10, learning_rate=0.01,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0.3,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFmodel.fit(train_X_new,train_Y)\n",
    "DTmodel.fit(train_X_new,train_Y)\n",
    "logreg.fit(train_X_new,train_Y)\n",
    "knn.fit(train_X_new,train_Y)\n",
    "xgbm.fit(train_X_new,train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Calculating Cross Validation Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that can be called to calculate cross validation score for each model \n",
    "def cross_val(X,x,y):\n",
    "    scores = cross_val_score(X, x, y, cv=5, scoring = \"roc_auc\")\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Cross Validation Score based on ROC_AUC by calling th ecross_val function defined before \n",
    "RF_score = round(cross_val(RFmodel,train_X_new, train_Y) * 100, 2)\n",
    "DT_score =round(cross_val(DTmodel,train_X_new, train_Y) * 100, 2)\n",
    "logreg_score = round(cross_val(logreg,train_X_new, train_Y) * 100, 2)\n",
    "knn_score = round(cross_val(knn,train_X_new, train_Y) * 100, 2)\n",
    "xgb_score = round(cross_val(xgbm,train_X_new, train_Y) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tabulating the CV Scores for Different Models** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB</td>\n",
       "      <td>78.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>75.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>69.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>56.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>55.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Score\n",
       "0                  XGB  78.70\n",
       "1  Logistic Regression  75.31\n",
       "2        Random_Forest  69.36\n",
       "4                   DT  56.06\n",
       "3                  KNN  55.68"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'Model': ['XGB',  'Logistic Regression', 'Random_Forest','KNN','DT'\n",
    "              ],'Score': [xgb_score , logreg_score,RF_score,knn_score,DT_score]})\n",
    "results = results.sort_values(['Score'], ascending=[False])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Hyperparameter Tuning for Logistic Regression\n",
    "\n",
    "Results from the previous step shows that XGBoost and Logistic Regression have the best CV scores (this was also found to be true when the iteration was repeated with 50,100 and 150 features). To reduce the possibility of overfitting, the GridSearchCV method from scikit is used to perform a hyper-paramter optimization on these two models to check if it improves the accuracy of prediction. Below is the code used for a hyperparamter search for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Grid Search to Optimize the Parameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 3.1622776601683795, 'penalty': 'l2'}\n",
      "accuracy : 0.902065\n"
     ]
    }
   ],
   "source": [
    "grid={\"C\":np.logspace(-2,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg1=LogisticRegression()\n",
    "logreg_hp=GridSearchCV(logreg1,grid,cv=3,verbose=0)\n",
    "logreg_hp.fit(train_X_new,train_Y)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_hp.best_params_)\n",
    "print(\"accuracy :\",logreg_hp.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Modeling with the Best Parameters from GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for logical regression after doing a GridSearchCV 75.36\n"
     ]
    }
   ],
   "source": [
    "logreg2=LogisticRegression(C=1000,penalty=\"l2\")#Grid Search Result  C:1000 l2 Accuracy :0.902095\n",
    "logreg2.fit(train_X_new,train_Y)\n",
    "logreg2_score = round(cross_val(logreg2,train_X_new, train_Y) * 100, 2)\n",
    "print(\"Accuracy for logical regression after doing a GridSearchCV: \", logreg2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below is commented out as Kaggle timed out before the grid search is complete for these paramters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbm_h = xgb.XGBClassifier(silent=False, scale_pos_weight=1,learning_rate=0.01, colsample_bytree = 0.4,subsample = 0.8,objective='binary:logistic', \n",
    "                    n_estimators=1000, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=4, \n",
    "                      gamma=10)\n",
    "\n",
    "\n",
    "#params = {\n",
    "#        'min_child_weight': [1, 5, 10],\n",
    "#        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "#        'subsample': [0.6, 0.8, 1.0],\n",
    "#        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "#xgbm_hp= GridSearchCV(estimator =  xgbm, param_grid = params, scoring='roc_auc',n_jobs=1,iid=False, cv=3)\n",
    "#xgbm_hp.fit(train_X_new,train_Y)\n",
    "#xgbm_hp.best_params_, xgbm_hp.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training XGB Model with Best Parameters from GridSearch **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbm2 = xgb.XGBClassifier(silent=1, \n",
    "#                      scale_pos_weight=1,\n",
    "#                     learning_rate=0.01,  \n",
    "#                      colsample_bytree = 0.4,\n",
    "#                      subsample = 0.8,\n",
    "#                      objective='binary:logistic', \n",
    "#                      n_estimators=1000, \n",
    "#                      reg_alpha = 0.3,\n",
    "#                      max_depth=4, \n",
    "#                      gamma=10)\n",
    "#xgbm2.fit(train_X_new,train_Y)\n",
    "#xgb2_score = round(cross_val(xgbm2,train_X_new, train_Y) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction on Test Data using the Best Performing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment out the models you wont use in this iteration\n",
    "#logreg_pred = logreg.predict_proba(test_X_new)[:,1]\n",
    "#logreg2_pred = logreg2.predict_proba(test_X_new)[:,1]\n",
    "xgb_pred =xgbm.predict_proba (test_X_new)[:,1]\n",
    "#xgb2_pred =xgbm2.predict_proba (test_X_new)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Generate Output File for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The lines below shows you how to save your data in the format needed to score it in the competition\n",
    "output = pd.DataFrame({'ID_code': test.ID_code,\n",
    "                      'target': xgb_pred})\n",
    "\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
